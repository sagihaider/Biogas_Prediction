{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_MISO.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiENvR3f8B2k20WsqvHX6z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/Biogas_Prediction/blob/master/MLP_MISO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6IJ4D2S_b3j",
        "outputId": "6152fe48-a533-4cf8-b8b2-900f3285d773"
      },
      "source": [
        "!git clone https://github.com/sagihaider/Biogas_Prediction.git \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from sklearn.preprocessing import normalize\n",
        "from keras import optimizers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Biogas_Prediction'...\n",
            "remote: Enumerating objects: 339, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 339 (delta 8), reused 46 (delta 1), pack-reused 275\u001b[K\n",
            "Receiving objects: 100% (339/339), 72.42 MiB | 36.59 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "59umuebs_wUj",
        "outputId": "f92f13b7-9112-474a-8047-8344ada42de4"
      },
      "source": [
        "data = pd.read_excel('/content/Biogas_Prediction/Data/Gasification Data_Daya.xlsx', index_col=0, header=0)\n",
        "data = data.iloc[1:223,:14] \n",
        "display(list(data.columns.values))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['C [%wt db]',\n",
              " 'H [%wt db]',\n",
              " 'O [%wt db]',\n",
              " 'Moisture [%wt]',\n",
              " 'Ash [%wt db]',\n",
              " 'ER [-]',\n",
              " 'T [ÂºC]',\n",
              " 'Bed material',\n",
              " 'Steam/Biomass',\n",
              " 'H2 [%vol N2 free]',\n",
              " 'CO [%vol N2 free]',\n",
              " 'CO2 [%vol N2 free]',\n",
              " 'CH4 [%vol N2 free]',\n",
              " 'GY [Nm3/kg daf]']"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egP1aM5__5T8",
        "outputId": "e7b09ed4-82b6-4904-a30b-e7c6c0d1c38f"
      },
      "source": [
        "# Drop null balues and store dataframe in dataframe 2\n",
        "data=data.dropna()\n",
        "print(data)\n",
        "\n",
        "#Check Null values again after removing\n",
        "print(data.isnull().values.any())\n",
        "print(data.isna().values.any())\n",
        "\n",
        "X_all = data.iloc[:, :9]\n",
        "y_all = data.iloc[:, 9:]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        C [%wt db] H [%wt db]  ...  CH4 [%vol N2 free] GY [Nm3/kg daf]\n",
            "Sr No.                         ...                                    \n",
            "2            50.00        5.7  ...            6.474820        2.500000\n",
            "3            50.00        5.7  ...            7.164179        2.500000\n",
            "4            50.00        5.7  ...            6.474820        2.100000\n",
            "5            50.00        5.7  ...            6.474820        2.400000\n",
            "6            50.00        5.7  ...            8.181818        2.100000\n",
            "...            ...        ...  ...                 ...             ...\n",
            "219          46.85        6.3  ...            9.706147        0.947532\n",
            "220          46.85        6.3  ...            9.455065        0.975752\n",
            "221          46.76       5.68  ...            9.933682        0.971784\n",
            "222          46.76       5.68  ...            8.778402        0.982989\n",
            "223          46.76       5.68  ...            8.534014        0.973816\n",
            "\n",
            "[187 rows x 14 columns]\n",
            "False\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_ZwSMVEAArd",
        "outputId": "520b81fb-1e38-4eaf-8d77-76606ae20829"
      },
      "source": [
        "\n",
        "input_columns = list(X_all.columns.values)\n",
        "input_columns = [i.split(' [', 1)[0] for i in input_columns]\n",
        "output_columns = list(y_all.columns.values)\n",
        "output_columns = [i.split(' [', 1)[0] for i in output_columns]\n",
        "print(input_columns, output_columns)\n",
        "\n",
        "Xnorm = normalize(X_all, 'l2', axis=1)\n",
        "ynorm = normalize(y_all, 'l2', axis=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['C', 'H', 'O', 'Moisture', 'Ash', 'ER', 'T', 'Bed material', 'Steam/Biomass'] ['H2', 'CO', 'CO2', 'CH4', 'GY']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkZFTJOziOoZ"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def coeff_determination(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXnSyFGkAtAf"
      },
      "source": [
        "# Create MISO using train & test split without R^2\n",
        "nrow, ncol=ynorm.shape\n",
        "\n",
        "for i in range(ncol):\n",
        "\n",
        "  X = Xnorm\n",
        "  y = ynorm[:,i]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(Xnorm, y, test_size=0.33)\n",
        "\n",
        "  # define the keras model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(10, input_shape = (9,), activation = 'sigmoid'))\n",
        "  model.add(Dense(10, activation = 'sigmoid'))\n",
        "  model.add(Dense(10, activation = 'sigmoid'))\n",
        "  model.add(Dense(1))\n",
        "  model.summary()\n",
        "\n",
        "  sgd = optimizers.SGD(lr = 0.01)    # stochastic gradient descent optimizer\n",
        "\n",
        "  model.compile(optimizer = sgd, loss = 'mean_squared_error', metrics = ['mse']) # for regression problems, mean squared error (MSE) is often employed\n",
        "  history=model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = 25, epochs = 20, verbose = 1)\n",
        "\n",
        "  # results = model.evaluate(X_test, y_test)\n",
        "  # print('loss: ', results[0])\n",
        "  # print('mse: ', results[1])\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  print(history.history.keys())\n",
        "  # list all data in history\n",
        "  print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['mse'])\n",
        "  plt.plot(history.history['val_mse'])\n",
        "  plt.title(output_columns[i])\n",
        "  plt.ylabel('MSE')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "  del model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho7_LOrIipow",
        "outputId": "f82b89a4-1127-4dd4-d5c5-798d3f4dda1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create MISO using train & test split without R^2\n",
        "rsqaure_list = []\n",
        "nrow, ncol=ynorm.shape\n",
        "\n",
        "for i in range(ncol):\n",
        "\n",
        "  X = Xnorm\n",
        "  y = ynorm[:,i]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(Xnorm, y, test_size=0.33)\n",
        "\n",
        "  # define the keras model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(10, input_shape = (9,), activation = 'sigmoid'))\n",
        "  model.add(Dense(10, activation = 'sigmoid'))\n",
        "  model.add(Dense(10, activation = 'sigmoid'))\n",
        "  model.add(Dense(1))\n",
        "  model.summary()\n",
        "\n",
        "  sgd = optimizers.SGD(lr = 0.01)    # stochastic gradient descent optimizer\n",
        "\n",
        "  model.compile(optimizer = sgd, loss = 'mean_squared_error', metrics = ['mse']) # for regression problems, mean squared error (MSE) is often employed\n",
        "  history=model.fit(X_train, y_train, batch_size = 25, epochs = 20, verbose = 1)\n",
        "  \n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred = y_pred.ravel()\n",
        "  r2=r2_score(y_test, y_pred)\n",
        "  print('The RSquare Value is:', r2)\n",
        "  rsqaure_list.append(r2)\n",
        "\n",
        "print(rsqaure_list)\n",
        "  "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_80 (Dense)             (None, 10)                100       \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 331\n",
            "Trainable params: 331\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.2722 - mse: 0.2722\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.1336 - mse: 0.1336\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0886\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0556 - mse: 0.0556\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0315 - mse: 0.0315\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0246 - mse: 0.0246\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0231 - mse: 0.0231\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0232 - mse: 0.0232\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0214 - mse: 0.0214\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0241 - mse: 0.0241\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0293 - mse: 0.0293\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0224 - mse: 0.0224\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0214 - mse: 0.0214\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38d6201170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "The RSquare Value is: -0.005446438190191971\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_84 (Dense)             (None, 10)                100       \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 331\n",
            "Trainable params: 331\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0608 - mse: 0.0608\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0347 - mse: 0.0347\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0204 - mse: 0.0204\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0192 - mse: 0.0192\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0175 - mse: 0.0175\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0146 - mse: 0.0146\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0161 - mse: 0.0161\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0205 - mse: 0.0205\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0168 - mse: 0.0168\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0161 - mse: 0.0161\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0193 - mse: 0.0193\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0178 - mse: 0.0178\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0157 - mse: 0.0157\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0182 - mse: 0.0182\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0157 - mse: 0.0157\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0194 - mse: 0.0194\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0180 - mse: 0.0180\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38d60ece60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "The RSquare Value is: -0.020651319838958848\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_88 (Dense)             (None, 10)                100       \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 331\n",
            "Trainable params: 331\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.3317 - mse: 0.3317\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.1662 - mse: 0.1662\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0857\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0575 - mse: 0.0575\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0434 - mse: 0.0434\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0347 - mse: 0.0347\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0300 - mse: 0.0300\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0347 - mse: 0.0347\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0263 - mse: 0.0263\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0288 - mse: 0.0288\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38c4ea58c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "The RSquare Value is: -0.010458367031016769\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_92 (Dense)             (None, 10)                100       \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 331\n",
            "Trainable params: 331\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.4541 - mse: 0.4541\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.2021 - mse: 0.2021\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0966 - mse: 0.0966\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0289 - mse: 0.0289\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0128 - mse: 0.0128\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0087 - mse: 0.0087\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0060 - mse: 0.0060\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0066 - mse: 0.0066\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0071 - mse: 0.0071\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0066 - mse: 0.0066\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0067 - mse: 0.0067\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0053 - mse: 0.0053\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0103 - mse: 0.0103\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0072 - mse: 0.0072\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0057 - mse: 0.0057\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0075 - mse: 0.0075\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38d61065f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "The RSquare Value is: -0.014794698081221025\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_96 (Dense)             (None, 10)                100       \n",
            "_________________________________________________________________\n",
            "dense_97 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_98 (Dense)             (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 331\n",
            "Trainable params: 331\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.3700 - mse: 0.3700\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.1372 - mse: 0.1372\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0494 - mse: 0.0494\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0186 - mse: 0.0186\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0067 - mse: 0.0067\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0028 - mse: 0.0028\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - mse: 0.0011\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 5.1776e-04 - mse: 5.1776e-04\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 3.8589e-04 - mse: 3.8589e-04\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 3.3266e-04 - mse: 3.3266e-04\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 2.4045e-04 - mse: 2.4045e-04\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 2.6684e-04 - mse: 2.6684e-04\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 2.8881e-04 - mse: 2.8881e-04\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 2.5601e-04 - mse: 2.5601e-04\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 2.4201e-04 - mse: 2.4201e-04\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 2.4760e-04 - mse: 2.4760e-04\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 2.0003e-04 - mse: 2.0003e-04\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.1248e-04 - mse: 2.1248e-04\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 2.4242e-04 - mse: 2.4242e-04\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 2.6648e-04 - mse: 2.6648e-04\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38c34af4d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "The RSquare Value is: 0.0005458617520592846\n",
            "[-0.005446438190191971, -0.020651319838958848, -0.010458367031016769, -0.014794698081221025, 0.0005458617520592846]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}